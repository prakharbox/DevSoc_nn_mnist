{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/pikapika88/mnist-classifier-nn?scriptVersionId=192087675\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pickle\nfrom sklearn.model_selection import train_test_split\n\nclass Layer:\n    def forward(self, inputs):\n        \"\"\"\n        Compute the forward pass.\n        \"\"\"\n        raise NotImplementedError\n\n    def backward(self, gradient):\n        \"\"\"\n        Compute the backward pass.\n        \"\"\"\n        raise NotImplementedError\n\nclass Linear(Layer):\n    \"\"\"Fully connected layer.\"\"\"\n\n    def __init__(self, input_size, output_size):\n        \"\"\"\n        Initialize the layer.\n        \"\"\"\n        self.weights = np.random.randn(input_size, output_size) * 0.01\n        self.biases = np.zeros((1, output_size))\n        self.inputs = None\n        self.gradients = {'weights': None, 'biases': None}\n\n    def forward(self, inputs):\n        \"\"\"Compute forward pass.\"\"\"\n        self.inputs = inputs\n        return np.dot(inputs, self.weights) + self.biases\n\n    def backward(self, gradient):\n        \"\"\"Compute backward pass.\"\"\"\n        self.gradients['weights'] = np.dot(self.inputs.T, gradient)\n        self.gradients['biases'] = np.sum(gradient, axis=0, keepdims=True)\n        return np.dot(gradient, self.weights.T)\n\nclass ReLU(Layer):\n    \"\"\"ReLU activation layer.\"\"\"\n    \n    def __init__(self):\n        self.inputs = None\n        \n    def forward(self, inputs):\n        \"\"\"Apply ReLU activation.\"\"\"\n        self.inputs = inputs\n        return np.maximum(0, inputs)\n\n    def backward(self, gradient):\n        \"\"\"Compute gradient of ReLU.\"\"\"\n        return gradient * (self.inputs > 0)\n\nclass Softmax(Layer):\n    \"\"\"Softmax activation layer.\"\"\"\n\n    def forward(self, inputs):\n        \"\"\"Apply Softmax activation.\"\"\"\n        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n        self.probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n        return self.probabilities\n\n    def backward(self, gradient):\n        \"\"\"Compute gradient of Softmax.\"\"\"\n        return gradient  # Gradient calculation is handled by CrossEntropyLoss\n\nclass Loss:\n    \"\"\"Base class for loss functions.\"\"\"\n\n    def forward(self, predictions, targets):\n        \"\"\"Compute the loss.\"\"\"\n        raise NotImplementedError\n\n    def backward(self, predictions, targets):\n        \"\"\"Compute the gradient of the loss.\"\"\"\n        raise NotImplementedError\n\nclass CrossEntropyLoss(Loss):\n    \"\"\"Cross-entropy loss function.\"\"\"\n\n    def forward(self, predictions, targets):\n        \"\"\"Compute cross-entropy loss.\"\"\"\n        eps = 1e-15  # To avoid log(0) errors\n        predictions = np.clip(predictions, eps, 1 - eps)\n        return -np.sum(targets * np.log(predictions)) / predictions.shape[0]\n\n    def backward(self, predictions, targets):\n        \"\"\"Compute gradient of cross-entropy loss.\"\"\"\n        eps = 1e-15\n        predictions = np.clip(predictions, eps, 1 - eps)\n        return (predictions - targets) / predictions.shape[0]\n\nclass SGD:\n    \"\"\"Stochastic Gradient Descent optimizer.\"\"\"\n\n    def __init__(self, learning_rate):\n        \"\"\"\n        Initialize the optimizer.\n        \"\"\"\n        self.learning_rate = learning_rate\n\n    def step(self, layer):\n        \"\"\"\n        Perform a single optimization step.\n        \"\"\"\n        layer.weights -= self.learning_rate * layer.gradients['weights']\n        layer.biases -= self.learning_rate * layer.gradients['biases']\n\nclass Model:\n    \"\"\"Neural network model.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the model.\"\"\"\n        self.layers = []\n        self.loss = None\n        self.optimizer = None\n\n    def add_layer(self, layer):\n        \"\"\"Add a layer to the model.\"\"\"\n        self.layers.append(layer)\n\n    def compile(self, loss, optimizer):\n        \"\"\"Compile the model.\"\"\"\n        self.loss = loss\n        self.optimizer = optimizer\n\n    def forward(self, inputs):\n        \"\"\"Perform forward pass through all layers.\"\"\"\n        for layer in self.layers:\n            inputs = layer.forward(inputs)\n        return inputs\n\n    def backward(self, gradient):\n        \"\"\"Perform backward pass through all layers.\"\"\"\n        for layer in reversed(self.layers):\n            gradient = layer.backward(gradient)\n\n    def train(self, X, y, epochs, batch_size):\n        \"\"\"Train the model.\"\"\"\n        for epoch in range(epochs):\n            epoch_loss = 0\n            indices = np.arange(X.shape[0])\n            np.random.shuffle(indices)\n            X = X[indices]\n            y = y[indices]\n\n            for i in range(0, len(X), batch_size):\n                X_batch = X[i:i+batch_size]\n                y_batch = y[i:i+batch_size]\n\n                predictions = self.forward(X_batch)\n                batch_loss = self.loss.forward(predictions, y_batch)\n                epoch_loss += batch_loss\n\n                gradient = self.loss.backward(predictions, y_batch)\n                self.backward(gradient)\n\n                for layer in self.layers:\n                    if hasattr(layer, 'weights') and hasattr(layer, 'biases'):\n                        self.optimizer.step(layer)\n\n            print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(X):.4f}\")\n\n    def predict(self, X):\n        \"\"\"Make predictions on new data.\"\"\"\n        return self.forward(X)\n\n    def evaluate(self, X, y):\n        \"\"\"Evaluate the model.\"\"\"\n        predictions = self.predict(X)\n        loss = self.loss.forward(predictions, y)\n        accuracy = np.mean(np.argmax(predictions, axis=1) == np.argmax(y, axis=1))\n        return loss, accuracy\n    \n    def save(self, filepath):\n        \"\"\"Save the model weights to a file.\"\"\"\n        weights = []\n        for layer in self.layers:\n            if hasattr(layer, 'weights') and hasattr(layer, 'biases'):\n                weights.append((layer.weights, layer.biases))\n        with open(filepath, 'wb') as f:\n            pickle.dump(weights, f)\n        print(f\"Weights saved to {filepath}\")\n\n    def load(self, filepath):\n        \"\"\"Load the model weights from a file.\"\"\"\n        with open(filepath, 'rb') as f:\n            weights = pickle.load(f)\n    \n        weight_index = 0  # Counter for the weights list\n    \n        for layer in self.layers:\n            if hasattr(layer, 'weights') and hasattr(layer, 'biases'):\n                try:\n                    layer.weights, layer.biases = weights[weight_index]\n                    weight_index += 1  # Increment the counter only if weights were loaded\n                except IndexError:\n                    print(f\"Error: Mismatch in the number of layers. Could not load weights for layer {layer}.\")\n                    break\n        print(f\"Weights loaded from {filepath}\")\n\n\ndef one_hot_encode(y, num_classes):\n    return np.eye(num_classes)[y.astype(int).reshape(-1)]\n\n# Load and preprocess data\ntraining_data = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\nX_train = training_data.iloc[:, 1:].values\ny_train = training_data.iloc[:, 0].values\n\nX_train = X_train.astype('float32') / 255\ny_train = y_train.astype('int')\n\n# One-hot encode the labels\ny_train = one_hot_encode(y_train, 10)\n\n# Initialize and compile the model\nmodel = Model()\nmodel.add_layer(Linear(784, 128))\nmodel.add_layer(ReLU())\nmodel.add_layer(Linear(128, 10))\nmodel.add_layer(Softmax())\n\nloss = CrossEntropyLoss()\noptimizer = SGD(learning_rate=0.01)\nmodel.compile(loss, optimizer)\n\n# Train the model\nmodel.train(X_train, y_train, epochs=20, batch_size=64)","metadata":{"_uuid":"043014c0-2207-4e54-970f-414c76a21a03","_cell_guid":"5e23a435-43cd-40dd-bd48-04563b692753","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-11T10:53:55.153011Z","iopub.execute_input":"2024-08-11T10:53:55.15346Z","iopub.status.idle":"2024-08-11T10:54:26.260168Z","shell.execute_reply.started":"2024-08-11T10:53:55.153413Z","shell.execute_reply":"2024-08-11T10:54:26.258915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\nX_test = np.array(test_data)\nX_test = X_test.astype('float32') / 255\ny_predicted = model.predict(X_test)\ny_predicted = np.argmax(y_predicted, axis=1)\nprint(y_predicted.T.shape)","metadata":{"_uuid":"88eed9e8-e620-48af-be1e-cdd1c86e8edb","_cell_guid":"41518511-89e7-44d9-a8fc-35febd769f42","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-11T10:58:23.484017Z","iopub.execute_input":"2024-08-11T10:58:23.484441Z","iopub.status.idle":"2024-08-11T10:58:25.388674Z","shell.execute_reply.started":"2024-08-11T10:58:23.484405Z","shell.execute_reply":"2024-08-11T10:58:25.387371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\nwith open('predictions_pre-trained.csv', 'w') as fileObj:\n    writerObj = csv.writer(fileObj)\n    writerObj.writerow(['ImageId','Label'])\n    for i in range(1,28001):\n        writerObj.writerow([i, y_predicted[i-1]])","metadata":{"_uuid":"01532174-7112-4aab-9b00-fd233d0d407c","_cell_guid":"aefe044c-1864-49cb-b93a-b1534a8dec5c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-11T10:58:57.467532Z","iopub.execute_input":"2024-08-11T10:58:57.467964Z","iopub.status.idle":"2024-08-11T10:58:57.501043Z","shell.execute_reply.started":"2024-08-11T10:58:57.467931Z","shell.execute_reply":"2024-08-11T10:58:57.49984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('/kaggle/working/model_weights')","metadata":{"execution":{"iopub.status.busy":"2024-08-11T10:54:45.281799Z","iopub.execute_input":"2024-08-11T10:54:45.282204Z","iopub.status.idle":"2024-08-11T10:54:45.290351Z","shell.execute_reply.started":"2024-08-11T10:54:45.282173Z","shell.execute_reply":"2024-08-11T10:54:45.288975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load('/kaggle/working/model_weights')","metadata":{"execution":{"iopub.status.busy":"2024-08-11T10:58:05.436529Z","iopub.execute_input":"2024-08-11T10:58:05.436973Z","iopub.status.idle":"2024-08-11T10:58:05.443976Z","shell.execute_reply.started":"2024-08-11T10:58:05.43694Z","shell.execute_reply":"2024-08-11T10:58:05.442588Z"},"trusted":true},"execution_count":null,"outputs":[]}]}